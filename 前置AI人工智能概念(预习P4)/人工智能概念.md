# 1. 人工智能概念
- 1.1 人工智能 包含 机器学习 包含 深度学习
  + 机器学习
  + 例子:识别身份证-->(弱人工智能)识别身份证号 310xxxxxxxxxx 训练如何识别图片中的身份证号码的识别器(图)
  + ![avatar](pic/身份证.png)
  + 深度学习(识别器的一种方式)
  + 神经网络:结构很深很复杂的时候称为深度学习
- 1.2 模式识别
  + 以身份证为例,某一个区域的某一个坐标来作为特征,分割,从图片中提取有用的数据特征
  + 做成一个向量样本,在进行训练
- 1.3 预测任务
  + 身份证识别出身份证号,也是属于预测任务
  + 已知的一张身份证的图片,需要获取我需要的未知信息,身份证号
	也算是一种预测任务
---
- 1.5 推荐算法
  + 基于内容的推荐
	根据人的某些历史数据(购买记录,聊天记录等)
	来推测出推荐给特定人群的信息
  + 例子:今日头条的新闻推送,会根据用户浏览过的新闻来推荐
- 1.6分布式
  + 数据量大了之后,将数据库(程序,算例) 分开到各个拆开的节点上来进行计算存储等
  + 逻辑:
    + 单机计算:通过map操作,本地将数据过滤(filter),转换等等简单的操作数据
    + 跨节点通信:聚合(reduce)操作,例:算所有数据的条目:本地的各个map数据分别统计之后进行全局的sum操作(图)
  + ![avatar](pic/分布式.png)
- 1.7容错性
	+ 分布式的节点可能会有宕机/磁盘坏了等丢失节点的时候,还能保证其他的节点还能进行运算,正常运行.
	+ 就代表系统能够有容错性
	+ 例:redis的缓存服务器
- 1.8协同过滤
	+ 结合推荐算法的例子
	    + 基于用户之间的共同兴趣的推荐
	    + 例:A,B在今日头条上都曾看过差不多相同的新闻,则今日头条以后也会将A看过的新闻来推荐给B,用户之间的共同兴趣的推荐(图)
        + ![avatar](pic/协同过滤.png)
# 2.获取数据与特征工程
- 2.1 数据集,训练集,测试集,验证集
	+ 训练过程:输入样本数据 (图),目的是做出一个分类器(机器学习是学习如何做分类器和使用分类器)
	+ 预测过程:根据训练过程的输出来进行预测,和训练相辅相成
      + 案例:y=ax 是个函数
      + x:代表人的属性,y:年龄属性
      + 已知用户的真实年龄,需要推测出准确的a参数
      + 减少误差的方式,枚举a.例如a≈100000->100000
      + 循环遍历,找到最优化最精确的a参数
	+ 数据集:(x,y)的数据样本(存储值),元数据
	+ 训练集:拿数据集用于找a参数的过程称为训练集,y=ax函数
	+ 测试集:已经找到过滤(filter)出来的a参数,这时候需要来测试这个分类器的效果的过程
	+ 验证集:属于训练集和测试集折中,用于防止已知数据准确,但是未知数据效果不好的情况
- 2.2 离散数据,连续数据
	+ 离散数据:
    	+ 例如年龄,30-32岁,数据集(30,31,32).但是不能再继续细化下去的数据称为离散数据
	+ 连续数据:
    	+ 例如体重,只要进度足够,体重的精度可以细化到无限的数据
- 2.3 特征,特征向量,样本,类标签(结合2.1的案例)
	+ 特征:人属性x中的属性身高,体重...都是属于特征
	+ 特征向量:把所有的特征属性组成数排列在一起的向量,身高,体重的排列就是一个二维向量
	+ 样本:例:人属性有小王,小张.那就是等于有2个样本.样本集合
	+ 类标签:年龄--就是类标签,预测的属性的说明
		
- 2.4 递归,迭代,并行
	+ 递归:例:二分查找(图).java中就是自己调用自己比如xml标签读取
		+ 树形搜索
		+ ![avatar](pic/二分查找.jpg)
	+ 迭代:
    	+ for循环
    	+ 比如轮询,如下数数,0-100
         ```
         ##0-100的轮询数数
         def countNum():
            for it range in 100:
                x=0
                print("第{}次轮询的数字为{}".format(x,x))
            return x
         ```
	+ 并行:
        + 多线程,函数分成多个线程同时去做
- 2.5 字段
    + 相当于2.1的案例中人属性的列(体重,身高...)
    + 有点类似数据库中(SQL)字段的意思
- 2.6 PCA,降维
    + 降维:将高维数据降低为低维数据
    + PCA:降维的一种方法(算法)
        + 例如 人属性(身高,体重,肤色),这个数据是有3个维度
        + 通过映射(PCA)算法数据变成($x_1,x_2$)这种二维数据
        + PS:这个二维数据是三维数据通过PCA的算法把数据维度降低,当然直接将三维数据中的标签
        __"肤色"__
        直接删除也是一种降维
- 2.7 分布,主题分布,长尾分布
	+ 分布: 帮助描述数据特性的方式,以2.1的案例为例
	+ ![avatar](pic/分布.png)
	+ 横轴是代表每一个人,纵轴是代表年龄段,画的直线统计的是20岁这个年龄段的正太分布
    ---
    + 主题分布:横轴是word(字段)或者doc(文档),用于看整体状态的,以下为主题分布的案例
    + 案例 1
      + ![avatar](pic/主题分布1.png)
        ```
        总共有10篇文档，分为两个主题。
        每个doc_topic[i]中包含了两个值，一个是主题0的概率，一个是主题1的概率。哪个概率大说明这个文档的主题是哪个。
        最终10篇文章分别对应于主题1, 1, 0, 0, 1, 1, 0, 0, 1, 0
        ```
      + ![avatar](pic/主题分布2.png)
    ---
    + 长尾分布:
      + 形状如下图,头重脚轻
      + ![avatar](pic/长尾分布.png) 
    ---
    + PS:分布还有其他的形式:正态分布,指数分布等等
- 2.8 矢量,标量
	+ 矢量
      + 向量,如(1,2),这种二维向量也称作矢量
	+ 标量
      + 常见的数,如1,2,10等等 
    + 区别:矢量是带方向的
- 2.9 多项式
    + 标量配合四则运算构造出来的一个式子
      + 例:$(x+1)^2$  ==>  多项式
- 2.10 归一化
    + 将数据的差异化统一权衡成一种数据
      + 例:2.1案例:人属性身高,体重.可以将这两个属性映射成身高与体重的比例
      + 比如可以用这个公式(身高cm－体重kg)×70﹪ 得出的比率来表达数值,将差异化的值归一化,也是一种降维操作
      + 防止某个属性对后面的操作影响特别大
- 2.11 人工标注
    + 1.1 见案例身份证识别案例
      + 识别出身份证号然后录入系统,没标签的身份证图片转换成识别出来之后带有标签的身份证图片的过程称为人工标注
- 2.12 特征选择
    + 引用2.1案例,体重,身高,肤色
    + 选特征,例如肤色对于预测用处不大,则删除肤色这个特征,作用也是减少特征的选择,有利于减少训练模型的复杂度
- 2.13 正例样本,负例样本
    + 二分类的一个过程
      + 比如:检测进程服务的状态,首先判断服务是否启动,启动的则判定为正例样本,未启动的判定为负例样本
- 2.14 类别集合,待分类项
    + 类别集合:案例2.13 服务启动和未启动的一种分类就是一种类别集合
    + 待分类项:案例2.1 预测人年龄的分类器已有,此时又有一段身高体重的数据来进入
      + 需要操作数据将其输入样本的分类器之后预测年龄,这些没有label(标签)的新进来的身高体重数据就是待分类项
- 2.15 序列,向量序列
    + 序列:一组数,比如(1,3,3,5)顺序不可调整的一组数称为序列
    + 向量序列:((80,160),(90,180),(100,190),(60,155)),同序列的概念,只是里面的数(标量)变成向量(矢量)
- 2.16 自变量,因变量
    + 以2.1的案例为例
      + $y=ax$
      + y:年龄(因变量)
      + x:身高体重...(自变量)
      + a:学习的参数
- 2.17 采样
    + 假如有10000人来做样本,随机从中选取100人来进行的过程就是采样
- 2.18 分词,分词器
    + 文本中分词
      + 例如:"高位数据投影到低维空间"
      + 将句子切成需要的词,|高位数据|投影|到|低维|空间
- 2.19 高位数据投影到低维空间
    + 引用2.1的案例,将身高体重肤色映射到坐标空间上(图)
    + ![avatar](pic/三维映射二维.png) 
- 2.20 特征提取
    + 从三个特征中提取出两个有用特征称为特征提取
      + 例:身高,体重,肤色,提取出体重,肤色两个特征的过程称为特征提取
- 2.21 近似
    + 举例:用户体重为120.2公斤
      + 近似取其体重为120公斤,核心的用处是加快运算时间,减少数据占用的空间
---
# 3. 模型训练
- 3.1 模型,参数
    + 举例2.1的$y=ax$的人属性案例
    + 这个式子(函数)就是一个预测模型
    + a这个未知的数需要预测的数就是参数
- 3.2 训练,预测,分类,回归,聚类
    + 训练:找到最优参数过程称为训练,以2.1的案例,找到a的过程称为训练
    + 预测:a已知,预测器已经有了.但是输入一些样本(身高体重,没有年龄),这个时候预测这些样本的年龄的过程称为预测
    + 分类:某些样本预测出来的年龄是个标签,假如说20-30岁是一档,31-40是一档,这种判断就是一种分类
    + 回归:如果预测值是体重(连续值)就是一个回归的过程),(训练一个回归器)
      + 回归通常是机器学习中使用的第一个算法。通过学习因变量和自变量之间的关系实现对数据的预测。
      + 例如，对房价估计时，需要确定房屋面积（自变量）与其价格（因变量）之间的关系，可以利用这一关系来预测给定面积的房屋的价格。可以有多个影响因变量的自变量。
      + 因此，回归有两个重要组成部分：自变量和因变量之间的关系，以及不同自变量对因变量影响的强度。
    + 聚类:样本都不知道年龄,但是能根据 身高,体重,肤色 这三个属性能区分出来哪些人属于一类,哪些人属于不同类的,而不需要年龄这个属性
- 3.3 目标函数,损失函数
    + 目标函数:构造的年龄预测器$y=ax$就是一个目标函数(预测函数)
    + 损失函数:评判年龄预测器函数中a参数是否是最优值的函数,例如:构建一个损失函数用来评价上面的构造的目标函数$(y_预-y_真)^2$ 度量参数是否是最优的
- 3.4 属性集X,目标属性Y
    + 属性集X:以2.1的案例来说,身高,体重,肤色这些属性将它们没有顺序的放到一起的集合,就称为属性集
    + 目标属性Y: age,就是所要求的的$y=ax$函数中的y,称为目标属性
- 3.5 梯度,步长
    + 梯度:a的参数移动方向,比如a的移动是从0-1000的方向,这个方向就成为梯度
    + 步长:加入每次a移动的长度为1,循环1000次,打到0-1000的迭代,这个距离长度1就称为步长
- 3.6 全局最优解,局部最优解
    + 如下图,假设最小值就是最优解
    + ![avatar](pic/局部和全局的最优解.png) 
- 3.7 准确度,误差,逼近
    + 准确度:比如:衡量2.1年龄预测器 $(y_真-y_预)^2$的方法
    + 误差:这个值越小,代表这个值的误差越小,这个式子也是求误差的算式,也代表这个这个预测值的准确度越高,也就是越接近3.6图中的最优解的点
    + 逼近:找到全局最优值的过程称为逼近,见3.6的抛物线图从抛物线的某一点找到min值的过程称为逼近
- 3.8 训练速度,解空间搜索
    + 训练速度:如其名,训练速度的快与慢
    + 解空间搜索:和3.7的逼近概念类似,往最优值方向搜索的过程称为解空间搜索
- 3.9 簇,聚类中心
    + 以2.1的年龄案例来说,先不看年龄,只看(身高,体重,肤色)这三个属性.期望将他们分到不同的堆(群体)中,没一个群体代表一个簇(样本群),如图所示
    + ![avatar](pic/簇.png) 
    + 聚类中心就是每个群体样本的中心点,也就是平均值
- 3.10 收敛
    + 以3.6的抛物线为例,当找到最优值的点之后,这个值不再作更新了,就称为收敛(值之后不再会有变化)
- 3.11 表征学习
    + 在深度学习提出之后提出来的
      + wiki补充:
      + 在特征学习算法出现之前，机器学习研究人员需要利用手动特征工程（manual feature learning）等技术从原始数据的领域知识（domain knowledge）建立特征，然后再部署相关的机器学习算法。虽然手动特征工程对于应用机器学习很有效，但它同时也是很困难、很昂贵、很耗时、并依赖于强大专业知识。特征学习弥补了这一点，它使得机器不仅能学习到数据的特征，并能利用这些特征来完成一个具体的任务。
      + 和预测性学习（Predictive Learning）不同，表征学习的目标不是通过学习原始数据预测某个观察结果，而是学习数据的底层结构（underlying structure），从而可以分析出原始数据的其它特性。表征学习允许计算机学习使用特征的同时，也学习如何提取特征：学习如何学习。在机器学习任务中，输入数据例如图片、视频、语言文字、声音等都是高维且冗余复杂，传统的手动提取特征已变得不切合实际，所以需要借助于优秀的特征学习技术。
- 3.12 非监督学习,监督学习
    + 非监督学习:没有明确的目标,只是把目标分类成簇(聚类问题,见3.9)
    + 监督学习:监督学习就是有明确的目标(标签),例如
  __识别身份证号__
- 3.13 BP神经网络,反向传播
    + BP神经网络:BP-->反向传播的意思,
      + 课外补充:神经网络的概念
      + BP神经网络是一种多层的前馈神经网络，其主要的特点是：信号是前向传播的，而误差是反向传播的。具体来说，对于如下的只含一个隐层的神经网络模型：
      + BP神经网络的过程主要分为两个阶段，
      + 第一阶段是信号的前向传播，从输入层经过隐含层，最后到达输出层；
      + 第二阶段是误差的反向传播，从输出层到隐含层，最后到输入层，依次调节隐含层到输出层的权重和偏置，输入层到隐含层的权重和偏置。
      + 如下图(3层BP神经网络):
      + ![avatar](pic/3层BP神经网络.png) 

- 3.14 输入层,隐藏层,输出层
    + 见3.13案例